{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Quicktorpedo/tp1_ml_ecc/blob/main/colabs/intro/Intro_to_Weights_%26_Biases.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNBvoJoKZAOQ"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/intro/Intro_to_Weights_&_Biases.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "<!--- @wandbcode{intro-colab} -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXmfNnn0ZAOR"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/intro/Intro_to_Weights_&_Biases.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "<!--- @wandbcode{intro-colab} -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLmk05AvZAOS"
      },
      "source": [
        "<img src=\"http://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n",
        "<!--- @wandbcode{intro-colab} -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLBvf4pmZAOS"
      },
      "source": [
        "Use [W&B](https://wandb.ai/site?utm_source=intro_colab&utm_medium=code&utm_campaign=intro) for machine learning experiment tracking, model checkpointing, collaboration with your team and more. See the full W&B Documentation [here](https://docs.wandb.ai/).\n",
        "\n",
        "In this notebook, you will create and track a machine learning experiment using a simple PyTorch model. By the end of the notebook, you will have an interactive project dashboard that you can share and customize with other members of your team. [View an example dashboard here](https://wandb.ai/wandb/wandb_example)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjjUoaG-ZAOS"
      },
      "source": [
        "## Prerequisites\n",
        "\n",
        "Install the W&B Python SDK and log in:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "66RehJ9CZAOS"
      },
      "outputs": [],
      "source": [
        "!pip install wandb -qU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Xm5N9F8dZAOT"
      },
      "outputs": [],
      "source": [
        "# Log in to your W&B account\n",
        "import wandb\n",
        "import random\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "H_FgGwzoZAOT",
        "outputId": "6205b2af-500d-457b-91f7-3122e39e6c3c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjosephogodja\u001b[0m (\u001b[33mjosephogodja-ecole-centrale-casablanca\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcFdy02yopfc"
      },
      "source": [
        "# TP1 Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pP_vwOqpOKk"
      },
      "source": [
        "## Importation des modules python adéquats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hZ2TP_4NotKT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from imblearn.over_sampling import SMOTE, ADASYN\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.metrics import (confusion_matrix, accuracy_score, precision_score,\n",
        "                             recall_score, f1_score, roc_curve, auc, r2_score)\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.neural_network import MLPClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_929SXepYxs"
      },
      "source": [
        "### Code principal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YeZahcu3paNz"
      },
      "outputs": [],
      "source": [
        "# 00 - Chargement du fichier CSV\n",
        "df = pd.read_csv(\"/content/uci-secom.csv\")\n",
        "\n",
        "# 01 - Pré-traitement des données\n",
        "\n",
        "# Suppression des colonnes avec plus de 5% de valeurs manquantes\n",
        "missing_values = df.isnull().sum()\n",
        "missing_percent = (missing_values / len(df)) * 100\n",
        "missing_data = missing_values[missing_values > 0].sort_values(ascending=False)\n",
        "missing_percent = missing_percent[missing_percent > 0].sort_values(ascending=False)\n",
        "\n",
        "drop_cols = missing_percent[missing_percent > 5].index\n",
        "df.drop(columns=drop_cols, inplace=True)\n",
        "\n",
        "# Suppression des lignes avec au moins une valeur manquante\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Suppression des doublons\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "# Suppression des colonnes ayant des valeurs nulles ou identiques\n",
        "cols_to_drop = [col for col in df.columns if df[col].nunique() <= 1]\n",
        "df.drop(columns=cols_to_drop, inplace=True)\n",
        "\n",
        "# Suppression de la colonne \"Time\" si elle existe\n",
        "df.drop(columns=['Time'], errors='ignore', inplace=True)\n",
        "\n",
        "# Encodage des variables catégoriques\n",
        "label_encoders = {}\n",
        "for col in df.select_dtypes(include=['object']).columns:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# Retrait des colonnes avec une corrélation supérieure à 0.9\n",
        "# Calcul la matrice de corrélation\n",
        "corr_matrix = df.corr()\n",
        "\n",
        "# Trouver les indices des colonnes avec une corrélation supérieure à 0.9\n",
        "upper_triangle = corr_matrix.where(abs(corr_matrix) > 0.9).stack().index.tolist()\n",
        "\n",
        "# Créer une liste de colonnes à supprimer\n",
        "columns_to_drop = []\n",
        "for col1, col2 in upper_triangle:\n",
        "    if col1 != col2:  # Si ce ne sont pas la même colonne\n",
        "        columns_to_drop.append(col2)\n",
        "\n",
        "# Supprimer les colonnes avec une corrélation supérieure à 0.9\n",
        "df_cleaned = df.drop(columns=columns_to_drop)\n",
        "\n",
        "# Retrait des valeurs aberrantes au delà de l'intervalle [-1250;1250]\n",
        "# Définition les seuils pour les valeurs aberrantes\n",
        "upper_limit = 1250\n",
        "lower_limit = -1250\n",
        "\n",
        "# Identifier les colonnes contenant des valeurs aberrantes\n",
        "columns_to_drop = []\n",
        "for col in df_cleaned.columns:\n",
        "    if (df_cleaned[col] > upper_limit).any() or (df_cleaned[col] < lower_limit).any():\n",
        "        columns_to_drop.append(col)\n",
        "\n",
        "# Supprimer les colonnes contenant des valeurs aberrantes\n",
        "df_cleaned_no_outliers = df_cleaned.drop(columns=columns_to_drop)\n",
        "\n",
        "# Création des vecteurs X (features) et Y (target)\n",
        "X = df_cleaned_no_outliers.drop(columns=['Pass/Fail'])\n",
        "Y = df_cleaned_no_outliers['Pass/Fail']\n",
        "\n",
        "# 02 - Analyse de composantes principale (ACP)\n",
        "\n",
        "# Standardisation des données\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Application de l'ACP\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# Variance expliquées par chaque composantes\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "# Cumul de la variance expliquée\n",
        "cumulative_variance = np.cumsum(explained_variance)\n",
        "\n",
        "# Choix du nombre de composantes pour conserver 95% de la variance expliquée\n",
        "threshold = 0.95\n",
        "n_components = np.argmax(cumulative_variance >= threshold) + 1\n",
        "print(f\"Nombre optimal de composantes pour {threshold*100}% de variance expliquée : {n_components}\")\n",
        "\n",
        "# Application de l'ACP avec le nombre de composantes jugées optimale\n",
        "pca = PCA(n_components=n_components)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# 03 - Tests de modèles d'IA\n",
        "\n",
        "# Division des données en train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pca, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tests de modèles\n",
        "\n",
        "# Suréchantillonnage avec SMOTE et ADASYN\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "adasyn = ADASYN(random_state=42)\n",
        "X_train_adasyn, y_train_adasyn = adasyn.fit_resample(X_train, y_train)\n",
        "\n",
        "# Modèles et hyperparamètres\n",
        "models = {\n",
        "    'DecisionTreeClassifier': GridSearchCV(DecisionTreeClassifier(), {'max_depth': [3, 5, 10, None]}, cv=5),\n",
        "    'RandomForestClassifier': GridSearchCV(RandomForestClassifier(), {'n_estimators': [50, 100, 200, 300], 'max_depth': [5, 10, None]}, cv=5),\n",
        "    'AdaBoostClassifier': GridSearchCV(AdaBoostClassifier(), {'n_estimators': [50, 100, 200]}, cv=5),\n",
        "    'MLPClassifier': GridSearchCV(MLPClassifier(max_iter=1000), {'hidden_layer_sizes': [(50,), (100,), (50, 50)], 'alpha': [0.0001, 0.001, 0.01]}, cv=5)\n",
        "}\n",
        "\n",
        "# Évaluation et journalisation avec WandB\n",
        "def evaluate_models(X_train, y_train, label, run_id):\n",
        "    results = {}\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for name, model in models.items():\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        results[name] = accuracy\n",
        "\n",
        "        # Sauvegarde du modèle\n",
        "        model_filename = f\"{name}_{label}_run{run_id}.pkl\"\n",
        "        joblib.dump(model, model_filename)\n",
        "        wandb.save(model_filename)\n",
        "\n",
        "        # Courbe ROC\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "        # Journalisation WandB\n",
        "        wandb.log({f\"{name}_{label}_accuracy\": accuracy, f\"{name}_{label}_AUC\": roc_auc})\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlabel('Taux de faux positifs (FPR)')\n",
        "    plt.ylabel('Taux de vrais positifs (TPR)')\n",
        "    plt.title(f'Courbes ROC - {label}')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Sauvegarde dans WandB\n",
        "    wandb.log({f\"ROC_{label}\": wandb.Image(plt)})\n",
        "\n",
        "    return results\n",
        "\n",
        "# Lancement de 5 expériences avec variations aléatoires\n",
        "total_runs = 5\n",
        "for run in range(total_runs):\n",
        "    wandb.init(\n",
        "        project=\"suivi-avec-wandb\",\n",
        "        name=f\"experiment_{run}\",\n",
        "        config={\n",
        "            \"learning_rate\": round(random.uniform(0.001, 0.1), 3),\n",
        "            \"pca_components\": 20,\n",
        "            \"dataset\": \"uci-secom\",\n",
        "            \"epochs\": 10\n",
        "        }\n",
        "    )\n",
        "\n",
        "    performance_original = evaluate_models(X_train, y_train, 'Original', run)\n",
        "    performance_smote = evaluate_models(X_train_smote, y_train_smote, 'SMOTE', run)\n",
        "    performance_adasyn = evaluate_models(X_train_adasyn, y_train_adasyn, 'ADASYN', run)\n",
        "\n",
        "    wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
